{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca971bc4",
   "metadata": {},
   "source": [
    "## Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e46a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d085e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe to store the data\n",
    "columns = ['Title',\n",
    " 'Author',\n",
    " 'Country',\n",
    " 'Date',\n",
    " 'Verified',\n",
    " 'Comment',\n",
    " 'Type Of Traveller',\n",
    " 'Seat Type',\n",
    " 'Route',\n",
    " 'Date Flown',\n",
    " 'Seat Comfort',\n",
    " 'Cabin Staff Service',\n",
    " 'Food & Beverages',\n",
    " 'Inflight Entertainment',\n",
    " 'Ground Service',\n",
    " 'Wifi & Connectivity',\n",
    " 'Value For Money',\n",
    " 'Recommended']\n",
    "\n",
    "df = pd.DataFrame(columns=columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5c40c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for getting data from one post\n",
    "def get_comment(soup):\n",
    "    data = {} # storing in one dictionary\n",
    "    # Title of the Comment\n",
    "    data[\"Title\"] = soup.find('h2', class_='text_header').text\n",
    "    # The Name of the commenter \n",
    "    data[\"Author\"] = soup.find('span', attrs={'itemprop': \"name\"}).text\n",
    "    \n",
    "    # The Country of the Commenter\n",
    "    # it's written in one element (need to scrape it without getting the text of child elements)\n",
    "    # also [1:-1] -> because country was written in parathesis\n",
    "    country = soup.find(\"h3\", class_='userStatusWrapper')\n",
    "    data[\"Country\"] = \"\".join(country.find_all(string=True, recursive=False)).strip()[1:-1]\n",
    "    \n",
    "    # Date of the comment (taken from datetime attribute of the time element)\n",
    "    data[\"Date\"] = soup.find('time')['datetime']\n",
    "\n",
    "    # extracting the comment\n",
    "    text_content = soup.find('div', class_='text_content')\n",
    "    # checking whether the comment is verified or not\n",
    "    try:\n",
    "        if text_content.find('strong').text.strip() == 'Trip Verified':\n",
    "            data[\"Verified\"] = True\n",
    "        else:\n",
    "            data[\"Verified\"] = False\n",
    "    except:\n",
    "        data['Verified'] = 'Not Specified'\n",
    "    # the comment itself same as country (without getting the text of the child element)\n",
    "    data[\"Comment\"] = \"\".join(text_content.find_all(string=True, recursive=False))\n",
    "    data[\"Comment\"] = data[\"Comment\"][data['Comment'].find(\"|\")+1:].strip()\n",
    "    \n",
    "    # getting the review stats from the table\n",
    "    # adding to the dictionary one by one\n",
    "    review_stats = soup.find('div', class_='review-stats')\n",
    "    review_stats = review_stats.find_all('tr')\n",
    "    for i in review_stats:\n",
    "        try:\n",
    "            data[i.find('td', class_='review-rating-header').text] = i.find('td', class_='review-value').text\n",
    "        except: \n",
    "            try:\n",
    "                star_num = 0\n",
    "                tds = i.find('td', class_='review-rating-stars')\n",
    "                tds = tds.find_all('span')\n",
    "                for td in tds:\n",
    "                    if 'fill' in td['class']:\n",
    "                        star_num +=1\n",
    "                data[i.find('td', class_='review-rating-header').text] = star_num\n",
    "            except:\n",
    "                data[i.find('td', class_='review-rating-header').text] = None\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7c7a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accessing the given url about british airways\n",
    "driver = webdriver.Chrome()\n",
    "page_num = 355\n",
    "\n",
    "while(True):\n",
    "    url = \"https://www.airlinequality.com/airline-reviews/british-airways/page/{}/\".format(page_num)\n",
    "    driver.get(url)\n",
    "    sleep(2)\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'html')\n",
    "\n",
    "    num_of_reviews = int(soup.find('div', class_='pagination-total').text.split(\" \")[-2])\n",
    "    comments = soup.find_all('div', class_='body')\n",
    "    for com in comments:\n",
    "        data = get_comment(com)\n",
    "        df.loc[len(df.index)] = data\n",
    "    if num_of_reviews <= len(df.index):\n",
    "        break\n",
    "    page_num+=1\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556b5f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd08c98",
   "metadata": {},
   "source": [
    "## Roberta Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e53e5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "# Preprocess text (username and link placeholders)\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "config = AutoConfig.from_pretrained(MODEL)\n",
    "# PT\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "#model.save_pretrained(MODEL)\n",
    "def sentiment(text):\n",
    "    text = preprocess(text)\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    output = model(**encoded_input)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    # # TF\n",
    "    # model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "    # model.save_pretrained(MODEL)\n",
    "    # text = \"Covid cases are increasing fast!\"\n",
    "    # encoded_input = tokenizer(text, return_tensors='tf')\n",
    "    # output = model(encoded_input)\n",
    "    # scores = output[0][0].numpy()\n",
    "    # scores = softmax(scores)\n",
    "    # Print labels and scores\n",
    "    ranking = np.argsort(scores)\n",
    "    ranking = ranking[::-1]\n",
    "    result = {}\n",
    "    for i in range(scores.shape[0]):\n",
    "        l = config.id2label[ranking[i]]\n",
    "        s = scores[ranking[i]]\n",
    "        result[l] = s\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cf4f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['positive'] = \"\"\n",
    "df['neutral'] = \"\"\n",
    "df['negative'] = \"\"\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    try:\n",
    "        results = sentiment(row['Comment'])\n",
    "        df.at[index, 'positive'] = results['positive']\n",
    "        df.at[index, 'neutral'] = results['neutral']\n",
    "        df.at[index, 'negative'] = results['negative']\n",
    "    except:\n",
    "        print(\"-------------\\n\"+ index +\" -> too long\\n ------------\")\n",
    "        continue\n",
    "    print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f6b97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"british_airways_sentiment.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b7ccec",
   "metadata": {},
   "source": [
    "# Topic Modeling (BERTopic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d08aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Text preprocessiong\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "# Topic model\n",
    "from bertopic import BERTopic\n",
    "\n",
    "# Dimension reduction\n",
    "from umap import UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052ebffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "print(f'There are {len(stopwords)} default stopwords. They are {stopwords}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed60335f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "df['review_without_stopwords'] = df['Comment'].apply(lambda x: ' '.join([w for w in x.split() if w.lower() not in stopwords]))\n",
    "\n",
    "# Lemmatization\n",
    "df['review_lemmatized'] = df['review_without_stopwords'].apply(lambda x: ' '.join([wn.lemmatize(w) for w in x.split() if w not in stopwords]))\n",
    "\n",
    "df.reset_index(inplace=True)\n",
    "# Take a look at the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95a2615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate UMAP\n",
    "umap_model = UMAP(n_neighbors=15, \n",
    "                  n_components=5, \n",
    "                  min_dist=0.0, \n",
    "                  metric='cosine', \n",
    "                  random_state=100)\n",
    "\n",
    "# Initiate BERTopic\n",
    "topic_model = BERTopic(umap_model=umap_model, language=\"english\", calculate_probabilities=True, nr_topics=15)\n",
    "\n",
    "# Run BERTopic model\n",
    "topics, probabilities = topic_model.fit_transform(df['review_lemmatized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78900067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of topics\n",
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c040c640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the topic predictions\n",
    "topic_prediction = topic_model.topics_[:]\n",
    "\n",
    "# Save the predictions in the dataframe\n",
    "df['topic_prediction'] = topic_prediction\n",
    "\n",
    "# Take a look at the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63f0f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic = topic_model.get_topic_info()\n",
    "for index, row in df_topic.iterrows():\n",
    "    df_topic.at[index, 'NameClear'] = \"-\".join(row['Name'].split('_')[1:])\n",
    "df_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488112d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = df.merge(df_topic, how='left', left_on='topic_prediction', right_on=\"Topic\")\n",
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c92e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_final = merged[merged['Topic'] != -1]\n",
    "merged_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7ab3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_final.to_excel(\"british_airways.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07273ea0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
